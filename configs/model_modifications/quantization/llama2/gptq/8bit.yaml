quantization_method: gptq
base_model: PLACEHOLDER
quantization_config:
  bits: 8
  batch_size: 16